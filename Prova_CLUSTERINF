library(readxl)
DB <- read_excel("C:/Users/User/OneDrive/Desktop/UniversitÃ /MAGISTRALE/MTBDA/Progetto/Esecuzione penale esterna per regioni 2023 snellito_PCA.xlsx")

df=DB%>%select(PC1,PC2)

km.res2 <- kmeans(df, 2)

fviz_cluster(km.res2, data = df)+theme_minimal()

fviz_nbclust(df, kmeans, method = "wss")

km.res <- kmeans(df, 2)
sil <- silhouette(km.res$cluster, dist(df))
fviz_silhouette(sil)

km.res.3 <- kmeans(df, 3)
sil <- silhouette(km.res.3$cluster, dist(df))
fviz_silhouette(sil)

km.res.4 <- kmeans(df, 5)
sil <- silhouette(km.res.4$cluster, dist(df))
fviz_silhouette(sil)

fviz_cluster(km.res.3, data = df)+theme_minimal()

aggregate(df, by=list(cluster=km.res.3$cluster), mean)

final_data <- cbind(df, kmean.c = km.res.3$cluster)

final_data$Region <- DB$Region



LETS NOW SEE THE HIERARCHICAL CLUSTERING

```{r}
#define linkage methods
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

#function to compute agglomerative coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

#calculate agglomerative coefficient for each clustering linkage method
sapply(m, ac)

```
WARD SEEMS THE BEST IN AGGLOMERATIVE COEFFICIENT

```{r}
#perform hierarchical clustering using Ward's minimum variance
clust <- agnes(df, method = "ward")

#produce dendrogram
pltree(clust, cex = 0.6, hang = -1, main = "Dendrogram") 
```
NOW WE NEED TO SELECT THE NUMBER OF CLUSTER (SIMILAR TO THE K IN THE K MEANS)
WE USE THE GAP STATISTICS
```{r}
#calculate gap statistic for each number of clusters (up to 5 clusters)
gap_stat <- clusGap(df, FUN = hcut, nstart = 25, K.max = 5, B = 50)

#produce plot of clusters vs. gap statistic
fviz_gap_stat(gap_stat)
```
THE HIGHEST GAP STASTISTICS IS IN K=3

```{r}
#compute distance matrix
d <- dist(df, method = "euclidean")
final_clust <- hclust(d, method = "ward.D2" )
groups <- cutree(final_clust, k=5)
plot(final_clust)
rect.hclust(final_clust, k = 5, border = "green")


final_data <- cbind(final_data, cluster = groups)
aggregate(final_data, by=list(cluster=final_data$cluster), mean)

ggplot(final_data) +
  aes(x = PC1, y = PC2, colour = as.factor(cluster)) +
  geom_point(shape = "circle", size = 1.5) +
  theme_minimal()

```
REMEMBER, THE DISTANCE MEASURE IS A MATRIX THAT REPRESENT THE (EUCLIDEAN) DISTANCE OF EACH UNIT TO THE OTHERS.
WE COULD SEE IT LIKE A NETWORK 
```{r}
library(igraph)
 g <- graph_from_adjacency_matrix(
    as.matrix(d),
    mode = "undirected",
    weighted = TRUE,
    diag = FALSE
  )
plot(g,
       edge.width = E(g)$weight/100,
       vertex.size = 10,
       vertex.label = NA,
       vertex.color = "lightblue",
       edge.color = "gray70",
)

```

DBScan

Selection of minpts: log(n)

Selection of EPS using k=datasetdimension + 1
```{r}
library(dbscan)
kNNdistplot(df, k = 3)
abline(h=1.4, col = "red", lty=2)
```
eps=0.55
```{r}
log(20)
dbscan.res <- dbscan(df, eps = 1.4, minPts = 3)
dbscan.res

hullplot(x = df, cl = dbscan.res) # grafico delle aree convesse
```
Is not working so well...
let try another eps
```{r}
log(20)
dbscan.res <- dbscan(df, eps = 1.3, minPts = 3)
dbscan.res

hullplot(x = df, cl = dbscan.res) 

final_data=cbind(final_data,dbscan.c=dbscan.res$cluster)
```
LET COMPARE THE 3 DIFFERENT CLUSTER RESULTS
```{r}
a=ggplot(final_data) +
  aes(x = PC1, y = PC2, colour = as.factor(kmean.c)) +
  geom_point(shape = "circle", size = 1.5) +
  theme_minimal()+
  labs(color = 'K-means')

b=ggplot(final_data) +
  aes(x = PC1, y = PC2, colour = as.factor(cluster)) +
  geom_point(shape = "square", size = 1.5) +
  theme_minimal()+
  labs(color = 'WARD')

c=ggplot(final_data) +
  aes(x = PC1, y = PC2, colour = as.factor(dbscan.c)) +
  geom_point(shape = "square", size = 1.5) +
  theme_minimal()+
  labs(color = 'DBScan')

library(cowplot)
a/b/c
```
